{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Research suggests that speech production can be modeled as a nonlinear dynamical system, wherein small perturbations in the interaction of its parts give rise to chaotic yet deterministic behavior. Parkinson's-related impairments (e.g. tremors, etc.) to the vocal organs, muscles and nerves can affect dynamics of the entire system, suggesting that nonlinear measures  may benefit the prediction of disease stage from voice recordings. \n",
    "\n",
    "## Features\n",
    "### Traditional measures:\n",
    "  * **shimmer** - extent of variation in amplitude from vocal cycle to vocal cycle\n",
    "  * **noise-to-harmonics ratio (NHR)** -  amplitude of noise relative to tonal components of speech signal\n",
    "  * **jitter** - measures pitch variation, such as vibrato and microtremor; calculated as differences in absolute frequencies of each cycle, averaged over a number of cycles\n",
    "      - *Note*: Natural pitch variation exists in healthy individuals, but may be perturbed in those with vocal impairments secondary to Parkinson's. \n",
    "  \n",
    "### Complex dynamical systems-based measures:\n",
    "\n",
    "  * **correlation dimension** - used to recreate all possible states (phase space) of the system that generates speech  \n",
    "  * **recurrence period density entropy (RPDE)** - this entropy measures the periodicity of the system\n",
    "      - When the signal deviates from its trajectory of recurring to the same point in the phase space, this may indicate a voice disorder. Many voice disorders impair the patient's ability to sustain vocal fold vibration, which can be measured as in terms of aperiodicity.  \n",
    "    \n",
    "  * **detrended fluctuation analysis (DFA)** - extent of stochastic self-similarity of noise in the speech signal\n",
    "      - Air blowing over vocal folds is a major cause of noise in speech, the pattern of which may be disrupted in some voice disorders. This noise can be characterized by a scaling exponent, which is higher in those with vocal disorders.  \n",
    "      \n",
    "  * **pitch period entropy (PPE)** - this entropy provides another measure of pitch variation (compare to **jitter**)\n",
    "      - Because pitch is produced and perceived on a logarithmic scale, PPE is calculated first by converting a pitch sequence to the logarithmic semitone scale. A filter then removes natural pitch variations (such as those due to gender and individual differences), and a probability distribution of voice variations is constructed. Finally, entropy is calculated, characterizing the extent of variation beyond natural fluctuations in pitch. Increased PPE may suggest speech variations beyond those seen in healthy speech production.\n",
    "      \n",
    "### Target feature:\n",
    "\n",
    "We will predict total scores on the [Unified Parkinson's disease rating scale (UPDRS)](https://neurosurgery.mgh.harvard.edu/functional/pdstages.htm), the scale most commonly used to study the long-term course of the disease. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import warnings \n",
    "import datetime\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "sns.set()\n",
    "pd.options.display.max_columns=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''column 1: Subject id \n",
    "\n",
    "colum 2-27: features \n",
    "features 1-5: Jitter (local),Jitter (local, absolute),Jitter (rap),Jitter (ppq5),Jitter (ddp), \n",
    "features 6-11: Shimmer (local),Shimmer (local, dB),Shimmer (apq3),Shimmer (apq5), Shimmer (apq11),Shimmer (dda), \n",
    "features 12-14: AC,NTH,HTN, \n",
    "features 15-19: Median pitch,Mean pitch,Standard deviation,Minimum pitch,Maximum pitch, \n",
    "features 20-23: Number of pulses,Number of periods,Mean period,Standard deviation of period, features 24-26: Fraction of locally unvoiced frames,Number of voice breaks,Degree of voice breaks \n",
    "\n",
    "column 28: UPDRS \n",
    "column 29: class information \n",
    "'''\n",
    "data_path = '../Data/classification/'\n",
    "col_names = \"subject_id,Jitter (local),Jitter (local. absolute),Jitter (rap),Jitter (ppq5),Jitter (ddp),Shimmer (local),Shimmer (local. dB),Shimmer (apq3),Shimmer (apq5), Shimmer (apq11),Shimmer (dda),AC,NTH,HTN,Median pitch,Mean pitch,Standard deviation,Minimum pitch,Maximum pitch,Number of pulses,Number of periods,Mean period,Standard deviation of period,Fraction of locally unvoiced frames,Number of voice breaks,Degree of voice breaks,UPDRS,class information\".split(',')\n",
    "df = pd.read_csv(data_path+\"train_data.txt\",names=col_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive statistics and data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "sns.heatmap(corr)\n",
    "plt.title(\"Heat map of the correlation of the features\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the people with and without disease is exactly half:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"class information\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we were unable to extract certain vocal features, we had to drop several columns and make do with what can extract.\n",
    "\n",
    "Apart from vocal features, we drop subject id, because we cannot use this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns_to_drop = \"subject_id,AC,NTH,HTN,Median pitch,Mean pitch,Standard deviation,Minimum pitch,Maximum pitch,Number of pulses,Number of periods,Mean period,Standard deviation of period,Fraction of locally unvoiced frames,Number of voice breaks,Degree of voice breaks\".split(',')\n",
    "#columns_to_drop = [\"subject_id\"]\n",
    "def to_target(val):\n",
    "    '''\n",
    "    Converting class 0 and 1 to False and true respectively\n",
    "    '''\n",
    "    if val==0:\n",
    "        return False\n",
    "    return True\n",
    "df.UPDRS = df.UPDRS.astype('float')\n",
    "df_new = df.drop(columns=columns_to_drop)\n",
    "\n",
    "df_new[\"class_information\"] = df_new[\"class information\"].apply(to_target)\n",
    "df_new.drop(columns=['class information'],inplace=True)\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(df_new[\"UPDRS\"])\n",
    "plt.title(\"Distribution of the UPDRS scores among the subjects\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that most subjects have a UPDRS score less than 5 while the rest are above that. This is because half of the data consists of subjects without parkinsons's disease(PD). We can consider a subject to not have PD if his UPDRS score is less than 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for z in df_new.drop(columns=['class_information','UPDRS']).columns:\n",
    "    ax = sns.violinplot(x=\"class_information\", y=z,  bw=.2, palette={True: \"#f9e9e8\", False: \"#CD5C5C\"}, inner=\"quartile\", data=df_new)\n",
    "    ax.set_xticklabels(['Not Diseased', 'Diseased'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(df_new,vars=df_new.drop(columns=['class_information',\"UPDRS\"]).columns, hue='class_information',palette={True: \"green\", False: \"blue\"})\n",
    "#g = sns.pairplot(df_new, vars=df_new.drop(columns='sex').columns, hue=\"sex\")\n",
    "g.map_diag(plt.hist)\n",
    "g.map_lower(sns.regplot,fit_reg=False)\n",
    "g.map_upper(sns.kdeplot, cmap=\"Blues_d\")\n",
    "#g.map_offdiag(sns.kdeplot, cmap=\"Blues_d\", n_levels=6);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets import the modules required for statistical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the target variable is a categorical variable, we can use the chi square test for determining the import features. However, for this we first need to convert the continuous variables into categorical variables. In order to do this, we shall put the values into bins. In order to get the best number of bins, we use the Freedman-Diaconis formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_,bins_edges = np.histogram(df_new.iloc[:,0],bins='fd')\n",
    "bins_edges,len(bins_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, we get 39 different bins with minimum loss of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a new dataframe for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = df_new.copy()\n",
    "for x in stats_df.drop(columns = [\"class_information\",\"UPDRS\"]).columns:\n",
    "    _,bins_edges = np.histogram(stats_df.loc[:,x],bins='fd')\n",
    "    bins_edges,len(bins_edges)\n",
    "    stats_df.loc[:,x] = pd.cut(stats_df.loc[:,x],bins=bins_edges).astype('str')\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have categorical variables, we can proceed with the chi square test of independence. The hypothesis is as follows:-\n",
    "* Null Hypothesis : There is no relationship between the two variables\n",
    "* Alternate hypothesis : There is a relationship between the two variables\n",
    "\n",
    "After performing the chi square test, we look at the p value and decide if the value is significant or not. We will go ahead with the standard confidence limit alpha = 0.05. So if p value is less than 0.05, then we can reject the null hypothesis.<br>\n",
    "Since there are multiple columns, lets build a simple wrapper function to perform the test on all the required columns at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def chi_square_wrapper(df,target,columns=None,verbose=False,alpha=0.05):\n",
    "    '''Function performs the Chi square test of independence of all the columns with the target variable.\n",
    "    This test is generally used for comparing two categorical variables.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df -> The pandas dataframe\n",
    "    target -> The target variable you want to test for\n",
    "    columns -> the specific list of columns you want test. All if not specified.\n",
    "    alpha\n",
    "    '''\n",
    "    if columns == None:\n",
    "        columns=[x for x in df.columns if df[x].dtype=='object']\n",
    "    for column in columns:\n",
    "        assert df[column].dtype == 'object',\"This function is intended for categorical indipendent variables\"\n",
    "    useful_columns = []\n",
    "    for column in columns:\n",
    "            if column == target:\n",
    "                continue\n",
    "            print(\"For \"+column+\" :-\")\n",
    "            cros = pd.crosstab(stats_df.loc[:,column],stats_df.loc[:,target])\n",
    "            res = chi2_contingency(cros)\n",
    "            if verbose:\n",
    "                print(res)\n",
    "            if res[1] < alpha :\n",
    "                print(\"Reject null hypothesis\\n--------------------\")\n",
    "                useful_columns.append(column)\n",
    "            else:\n",
    "                print(\"Accept null hypothesis\\n--------------------\")\n",
    "    print(\"According to the test, the useful columns are :-\\n\"+\", \".join(useful_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets perform the test and see which columns are useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_square_wrapper(stats_df,'class_information')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Chi square test shows these columns are significant. Now lets proceed to build the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier,AdaBoostClassifier,GradientBoostingClassifier,VotingClassifier,RandomForestClassifier,BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer, Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are certain preprocessing steps that help some model perform better. Lets create a wrapper for the pipeline creation for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipe_model(model,scale=preprocessing.Normalizer()):\n",
    "    '''\n",
    "    Returns a pipeline with the 4 steps:-\n",
    "    1)Scaling the data\n",
    "    2)Feature selection or dimentionality reducion\n",
    "    3)Regression\n",
    "    '''\n",
    "    return Pipeline([\n",
    "                  ('scale',scale),\n",
    "                  ('feature_selection', SelectFromModel(model)),\n",
    "                  ('Classification', model)\n",
    "                    ])\n",
    "def run_model(pipe,param_dict,verbose=True,n_jobs=-1,n_iter=100):\n",
    "    '''\n",
    "    pipe -> The pipeline or the model for which we want the randomised search to run\n",
    "    param_dict -> The dictionary of parameters.\n",
    "    Note : for parameter of a particular stage of a pipleline, use stage_name__param_name as the key. For example Classification__kernel for the classification stage kernel parameter\n",
    "    '''\n",
    "    clf = RandomizedSearchCV(pipe,param_dict,verbose=verbose,n_jobs=n_jobs,error_score=0.0,n_iter=n_iter).fit(X_train,y_train)\n",
    "    print(\"Training score : \",clf.score(X_train,y_train))\n",
    "    print(\"Testing score : \",clf.score(X_test,y_test))\n",
    "    print(\"Best params : \",clf.best_params_)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the pipline wrapper is ready, lets segregate the features and targets and create the training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets=df_new['class_information']\n",
    "features = df_new.drop(columns=['class_information','UPDRS'])\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stratify option allows to get equal split of classes in training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(features,targets,random_state=2,test_size=0.2,stratify=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = {}  #stores all the models\n",
    "scalers_to_test = [StandardScaler(), RobustScaler(), QuantileTransformer(),Normalizer(),None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that the features are ready, let's start building the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models considered are:-\n",
    "<ol>\n",
    "    <li> Naive Bayes </li>\n",
    "    <li> Logistic Regression </li>\n",
    "    <li> Decision Tree</li>\n",
    "    <li> Nearest Neighbours</li>\n",
    "    <li> SVM </li> \n",
    "    <li> Neural Networks </li>\n",
    "</ol>\n",
    "\n",
    "### And some ensembles such as :-\n",
    "<ol>\n",
    "    <li> Random Forest</li>\n",
    "    <li> Extra trees</li>\n",
    "    <li> Ada boost</li>\n",
    "    <li> Gradient boost</li>\n",
    "    <li> Bagging estimators</li>\n",
    "    <li> XG boost </li>        \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the models, we will use the randomized search to find the best possible hyperparameters. It is computationally expensive but we will not miss out on any model beacuse of poor hyperparameter tuning. Let's also calculate the total time taken to train the models using this method. Time taken for individual models is also displayed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "start_time = time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "feature_selection = [SelectFromModel(model),None]+[PCA(x) for x in range(1,11,3)]\n",
    "params = {\n",
    "    'scale' : scalers_to_test,\n",
    "    'feature_selection' : feature_selection\n",
    "}\n",
    "pipe = create_pipe_model(model)\n",
    "clfs[\"Naive_Bayes\"] = run_model(pipe,params,n_iter=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "feature_selection = [SelectFromModel(model),None]+[PCA(x) for x in range(1,11,3)]\n",
    "params = {\n",
    "    'scale' : scalers_to_test,\n",
    "    'feature_selection' : feature_selection,\n",
    "    'Classification__penalty' : ['l1','l2'],\n",
    "    'Classification__C' : [x for x in range(1,5)],\n",
    "    'Classification__solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "}\n",
    "pipe = create_pipe_model(model)\n",
    "clfs[\"Logistic_regression\"] = run_model(pipe,params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "feature_selection = [SelectFromModel(model),None]+[PCA(x) for x in range(1,11,3)]\n",
    "pipe = create_pipe_model(model)\n",
    "params = {\n",
    "    'scale' : scalers_to_test,\n",
    "    'feature_selection' : feature_selection,\n",
    "    'Classification__criterion' : ['gini','entropy'],\n",
    "    'Classification__splitter' : [\"best\",'random'],\n",
    "    'Classification__max_depth' : [x for x in range(8,20,3)]+[None],\n",
    "    'Classification__max_features' : ['auto','sqrt','log2',None],\n",
    "    'Classification__min_samples_split' : [x for x in range(2,10,2)],\n",
    "}\n",
    "clfs[\"Decision_tree\"] = run_model(pipe,params,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier()\n",
    "feature_selection = [SelectFromModel(model),None]+[PCA(x) for x in range(1,11,3)]\n",
    "pipe = create_pipe_model(model)\n",
    "params = {\n",
    "    'scale' : scalers_to_test,\n",
    "    'feature_selection' : feature_selection,\n",
    "    'Classification__n_neighbors' : [x for x in range(1,10,2)],\n",
    "    'Classification__algorithm' : ['auto', 'ball_tree', 'brute','kd_tree']\n",
    "}\n",
    "clfs[\"K_nearest\"] = run_model(pipe,params,verbose=True)  #Setting n_jobs as 6 so that i can continue working on my system without any lag:P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(probability=True)\n",
    "feature_selection = [None]+[PCA(x) for x in range(1,11,3)]\n",
    "pipe = create_pipe_model(model)\n",
    "params = {\n",
    "    'scale' : scalers_to_test,\n",
    "    'feature_selection' : feature_selection,\n",
    "    'Classification__C' : [x for x in range(1,10,2)],\n",
    "    'Classification__kernel' : ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "}\n",
    "clfs[\"SVM\"] = run_model(pipe,params,verbose=True,n_iter=10,n_jobs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier()\n",
    "feature_selection = [None]+[PCA(x) for x in range(1,11,3)]\n",
    "pipe = create_pipe_model(model)\n",
    "params = {\n",
    "    'scale' : scalers_to_test,\n",
    "    'feature_selection' : feature_selection,\n",
    "    'Classification__hidden_layer_sizes' : [(100,200,250,400),(100,),(200,360,300)],\n",
    "    'Classification__activation': [\"logistic\", \"relu\", \"tanh\",'identity'],\n",
    "}\n",
    "clfs[\"Neural_networks\"] = run_model(pipe,params,verbose=True,n_iter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "feature_selection = [SelectFromModel(model),None]+[PCA(x) for x in range(1,11,3)]\n",
    "pipe = create_pipe_model(model)\n",
    "params = {\n",
    "    'scale' : scalers_to_test,\n",
    "    'feature_selection' : feature_selection,\n",
    "    'Classification__n_estimators' : [10,100,500,1000,1500,2000,3000]#[x for x in range(10,5000,100)],\n",
    "    'Classification__criterion' : ['gini','entropy'],\n",
    "    'Classification__max_depth' : [x for x in range(8,15,4)]+[None],\n",
    "    'Classification__max_features' : ['auto','sqrt','log2',None],\n",
    "    'Classification__bootstrap' : [True,False],\n",
    "}\n",
    "clfs[\"Random_forest\"] = run_model(pipe,params,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier()\n",
    "feature_selection = [SelectFromModel(model),None]+[PCA(x) for x in range(1,11,3)]\n",
    "pipe = create_pipe_model(model)\n",
    "params = {\n",
    "    'scale' : scalers_to_test,\n",
    "    'feature_selection' : feature_selection,\n",
    "    'Classification__n_estimators' : [10,100,500,1000,1500,2000,3000],\n",
    "    'Classification__criterion' : ['gini','entropy'],\n",
    "    'Classification__max_depth' : [x for x in range(8,15,4)]+[None],\n",
    "    'Classification__max_features' : ['auto','sqrt','log2',None],\n",
    "    'Classification__bootstrap' : [True,False],\n",
    "}\n",
    "clfs[\"Extra_trees\"] = run_model(pipe,params,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADA Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AdaBoostClassifier()\n",
    "feature_selection = [SelectFromModel(model),None]+[PCA(x) for x in range(1,11,3)]\n",
    "pipe = create_pipe_model(model)\n",
    "params = {\n",
    "    'scale' : scalers_to_test,\n",
    "    'feature_selection' : feature_selection,\n",
    "    'Classification__n_estimators' : [10,100,500,1000,1500,2000,3000],\n",
    "    'Classification__base_estimator' : [GaussianNB(),LogisticRegression(),KNeighborsClassifier(),DecisionTreeClassifier(),SVC()],\n",
    "}\n",
    "clfs[\"Ada_boost\"] = run_model(pipe,params,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier()\n",
    "feature_selection = [SelectFromModel(model),None]+[PCA(x) for x in range(1,11,3)]\n",
    "pipe = create_pipe_model(model)\n",
    "params = {\n",
    "    'scale' : scalers_to_test,\n",
    "    'feature_selection' : feature_selection,\n",
    "    'Classification__n_estimators' : [10,100,500,1000,1500,2000,3000],\n",
    "    'Classification__loss' : ['deviance', 'exponential'],\n",
    "    'Classification__max_depth' : [x for x in range(3,15,4)]\n",
    "}\n",
    "clfs[\"Gradient_boost\"] = run_model(pipe,params,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaggingClassifier()\n",
    "feature_selection = [SelectFromModel(model),None]+[PCA(x) for x in range(1,11,3)]\n",
    "pipe = create_pipe_model(model)\n",
    "params = {\n",
    "    'scale' : scalers_to_test,\n",
    "    'feature_selection' : feature_selection,\n",
    "    'Classification__n_estimators' : [10,100,500,1000,1500,2000,3000],\n",
    "    'Classification__base_estimator' : [GaussianNB(),LogisticRegression(),KNeighborsClassifier(),DecisionTreeClassifier(),SVC()],\n",
    "}\n",
    "clfs[\"Ada_boost\"] = run_model(pipe,params,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG BOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier()\n",
    "feature_selection = [SelectFromModel(model),None]+[PCA(x) for x in range(1,11,3)]\n",
    "pipe = create_pipe_model(model)\n",
    "params = {\n",
    "    'scale' : scalers_to_test,\n",
    "    'feature_selection' : feature_selection,\n",
    "    'Classification__n_estimators' : [10,100,500,1000,1500,2000,3000],\n",
    "    'Classification__max_depth' : [x for x in range(3,15,4)],\n",
    "    'Classification__booster' : ['gbtree', 'gblinear','dart']\n",
    "}\n",
    "clfs[\"XGboost\"] = run_model(pipe,params,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = VotingClassifier([('gb',GaussianNB()),('lr',LogisticRegression()),('knn',KNeighborsClassifier()),('dt',DecisionTreeClassifier()),('svm',SVC(probability=True))])\n",
    "feature_selection = [SelectFromModel(model),None]+[PCA(x) for x in range(1,11,3)]\n",
    "pipe = create_pipe_model(model)\n",
    "params = {\n",
    "    'scale' : scalers_to_test,\n",
    "    'feature_selection' : feature_selection,\n",
    "    'Classification__voting' : ['hard','soft'],\n",
    "    'Classification__estimators':[[('gb',GaussianNB()),('lr',LogisticRegression()),('knn',KNeighborsClassifier()),('dt',DecisionTreeClassifier()),('svm',SVC(probability=True))]]\n",
    "}\n",
    "#clfs[\"Voting_classifier\"] = run_model(pipe,params,verbose=True)\n",
    "Voting_classifier = run_model(pipe,params,verbose=True,n_iter=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally,\n",
    "### Let's try the ensemble of all the classifier considered so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ensemble(object):\n",
    "    \"\"\"Stripped-down version of VotingClassifier that uses prefit estimators\"\"\"\n",
    "    def __init__(self, estimators, voting='hard', weights=None):\n",
    "        self.estimators = [e[1] for e in estimators]\n",
    "        self.named_estimators = dict(estimators)\n",
    "        self.voting = voting\n",
    "        self.weights = weights\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict class labels for X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        maj : array-like, shape = [n_samples]\n",
    "            Predicted class labels.\n",
    "        \"\"\"\n",
    "\n",
    "        #check_is_fitted(self, 'estimators')\n",
    "        if self.voting == 'soft':\n",
    "            maj = np.argmax(self.predict_proba(X), axis=1)\n",
    "\n",
    "        else:  # 'hard' voting\n",
    "            predictions = self._predict(X)\n",
    "            maj = np.apply_along_axis(lambda x:\n",
    "                                      np.argmax(np.bincount(x,\n",
    "                                                weights=self.weights)),\n",
    "                                      axis=1,\n",
    "                                      arr=predictions.astype('int'))\n",
    "        return maj\n",
    "\n",
    "    def _collect_probas(self, X):\n",
    "        \"\"\"Collect results from clf.predict calls. \"\"\"\n",
    "        return np.asarray([clf.predict_proba(X) for clf in self.estimators])\n",
    "\n",
    "    def _predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities for X in 'soft' voting \"\"\"\n",
    "        if self.voting == 'hard':\n",
    "            raise AttributeError(\"predict_proba is not available when\"\n",
    "                                 \" voting=%r\" % self.voting)\n",
    "        avg = np.average(self._collect_probas(X), axis=0, weights=self.weights)\n",
    "        return avg\n",
    "\n",
    "    @property\n",
    "    def predict_proba(self):\n",
    "        \"\"\"Compute probabilities of possible outcomes for samples in X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        avg : array-like, shape = [n_samples, n_classes]\n",
    "            Weighted average probability for each class per sample.\n",
    "        \"\"\"\n",
    "        return self._predict_proba\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Return class labels or probabilities for X for each estimator.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        If `voting='soft'`:\n",
    "          array-like = [n_classifiers, n_samples, n_classes]\n",
    "            Class probabilities calculated by each classifier.\n",
    "        If `voting='hard'`:\n",
    "          array-like = [n_samples, n_classifiers]\n",
    "            Class labels predicted by each classifier.\n",
    "        \"\"\"\n",
    "        if self.voting == 'soft':\n",
    "            return self._collect_probas(X)\n",
    "        else:\n",
    "            return self._predict(X)\n",
    "\n",
    "    def _predict(self, X):\n",
    "        \"\"\"Collect results from clf.predict calls. \"\"\"\n",
    "        return np.asarray([clf.predict(X) for clf in self.estimators]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ensemble([(a,b) for a,b in clfs.items()])\n",
    "yhat = model.predict(X_test)\n",
    "y = y_test\n",
    "print(\"Hard voting accuracy :-\",accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do :-\n",
    "* **Visualize results**\n",
    "* **GUI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_ml import ConfusionMatrix\n",
    "cm = ConfusionMatrix(list(y),list(model.predict(X_test)))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showFeatureImportance(model):\n",
    "    #FEATURE IMPORTANCE\n",
    "    # Get Feature Importance from the classifier\n",
    "    feature_importance = model.feature_importances_\n",
    "\n",
    "    # Normalize The Features\n",
    "    feature_importance = 100.0 * (feature_importance / Feature_importance.max())\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "    #plot relative feature importance\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.barh(pos, feature_importance[sorted_idx], align='center', color='#7A68A6')\n",
    "    plt.yticks(pos, np.asanyarray(X_cols)[sorted_idx])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
