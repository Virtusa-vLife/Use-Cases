{"cells":[{"metadata":{},"cell_type":"markdown","source":"# vLife Virtusa\n## Behavioural Assessment of Patient Health\n### Usecase Description\n_Solution to build a predictive model in order to asses patient health based on its behavioural assesment. Powered with Logistic Regression binary classification Model for predicting wheather individual is in Good or Poor health.Problem Statement we’ll solve is a binary classification task with the goal of predicting an individual’s health_.\n### Data Source\n- [Click Here for Data Source](https://www.kaggle.com/cdc/behavioral-risk-factor-surveillance-system)\n- [Click Here to view BRFSS Handbook](https://www.cdc.gov/brfss/annual_data/2015/pdf/codebook15_llcp.pdf)\n\n### Dataset Description\n- This dataset was collected by the Centers for Disease Control and Prevention.\n- Each year contains a few hundred columns. Please see one of the annual code books for complete details.\n- CSV files were converted from a SAS data format using pandas; there may be some data artifacts as a result.\n\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd \n\nimport datetime\nimport pickle\nfrom time import strftime\n\nfrom sklearn.metrics import accuracy_score,precision_score,roc_curve,roc_auc_score,classification_report\nfrom sklearn.model_selection import GridSearchCV,KFold,train_test_split,learning_curve\nfrom sklearn.preprocessing import LabelEncoder,StandardScaler,MinMaxScaler\nimport seaborn as se\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\nimport seaborn as sns\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import ExtraTreeClassifier\nfrom sklearn.linear_model import RidgeClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/behavioral-risk-factor-surveillance-system/2015.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of Dataset {}'.format(df.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in df.columns: \n    print(col) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(df.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis\n### Data Preprocessing & Generating Target variable"},{"metadata":{},"cell_type":"markdown","source":"- As per the Hand book **_RFHLTH** is our Target column denoting Adults with good or better health"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['_RFHLTH'].value_counts()\ndf['_RFHLTH'] = df['_RFHLTH'].replace({2: 0})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['_RFHLTH'].value_counts()\ndf = df.loc[df['_RFHLTH'].isin([0, 1])].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.rename(columns = {'_RFHLTH': 'Health'})\ndf['Health'] = df['Health'].astype('int')\ndf['Health'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"M = df[(df['Health'] == 0)]\nB = df[(df['Health'] == 1)]\ntrace = go.Bar(x = (len(M), len(B)), y = ['0','1'], orientation = 'h', opacity = 0.8, marker=dict(\n        color=['blue','grey'],\n        line=dict(color='#000000',width=1.5)))\n\nlayout = dict(title =  'Count of Health variable')\n                    \nfig = dict(data = [trace], layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"percent_missing = (df.isnull().sum() / len(df)).sort_values(ascending = False)\npercent_missing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_feat = pd.DataFrame(len(df['SEQNO']) - df.isnull().sum(), columns = ['Count'])\n\ntrace = go.Bar(x = null_feat.index, y = null_feat['Count'] ,opacity = 0.8, marker=dict(color = 'lightblue',\n        line=dict(color='#000000',width=1.5)))\n\nlayout = dict(title =  \"Missing Values\")\n                    \nfig = dict(data = [trace], layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **As per the BFSS Handbook there are 330 features in dataset filtering out features based on domain knowledge and kaggle kernals.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"filtered_df = df[['MENTHLTH','_AGEG5YR', 'SEX','EDUCA','EMPLOY1','INCOME2','_RACE','NUMADULT','MARITAL','VETERAN3','PREGNANT','ADPLEASR','ADDOWN','ADSLEEP','ADENERGY','ADEAT1','ADFAIL','ADTHINK','ADMOVE','Health']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filtered_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in filtered_df.columns.values:\n    print(f\" Unique values of {column} : {filtered_df[column].nunique()}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))\n\nmissing_data(filtered_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def impute_df(filtered_df):\n    #Changing values from 77,88,7,8,9,14 to NAN | Also, changing 88 values to 0\n    for x in ['ADPLEASR','ADDOWN','ADSLEEP','ADENERGY','ADEAT1','ADFAIL','ADTHINK','ADMOVE']:\n        filtered_df[x].replace(77, np.NaN, inplace= True)\n        filtered_df[x].replace(99, np.NaN, inplace= True)\n        filtered_df[x].replace(88, 0, inplace= True)\n\n\n    for x in ['EDUCA','EMPLOY1','_RACE', 'MARITAL']:\n        filtered_df[x].replace(9, np.NaN, inplace=True)\n\n    for x in ['VETERAN3','PREGNANT']:\n        filtered_df[x].replace(9, np.NaN, inplace= True)\n        filtered_df[x].replace(7, np.NaN, inplace= True)\n\n    filtered_df['_AGEG5YR'].replace(14, np.NaN, inplace= True)\n    filtered_df['INCOME2'].replace(77, np.NaN, inplace= True)\n    filtered_df['INCOME2'].replace(99, np.NaN, inplace= True)\n    filtered_df['MENTHLTH'].replace(88, 0, inplace= True)\n    \n    return filtered_df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imputed_df = impute_df(filtered_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imputed_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imputed_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ = imputed_df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Dropping NaN with no qestions answered for Motion features\nclean_data = filtered_df.dropna(subset=['ADPLEASR','ADDOWN','ADSLEEP','ADENERGY','ADEAT1','ADFAIL','ADTHINK','ADMOVE'],how='all')\nprint(clean_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data(clean_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def napreg(x):\n    if x['SEX'] == 1 or (x['_AGEG5YR'] >= 6 and x['_AGEG5YR'] <= 13):\n        return 2\n    else:\n        return x['PREGNANT']\n\nclean_data['PREGNANT'] = df.apply(napreg, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_df = clean_data.dropna(subset=['_AGEG5YR', 'SEX','EDUCA','EMPLOY1','INCOME2','_RACE','NUMADULT','MARITAL','VETERAN3','PREGNANT'])\nprint(cleaned_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_df = cleaned_df.dropna(subset=['ADPLEASR','ADDOWN','ADSLEEP','ADENERGY','ADEAT1','ADFAIL','ADTHINK','ADMOVE'],how='all')\ncleaned_df = cleaned_df.reset_index(drop=True)\nprint(cleaned_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data(cleaned_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = cleaned_df.dropna().reset_index(drop=True)\nprint(train_data.head(5))\ntrain_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data(train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_data.drop('Health',1)\ny = train_data.Health\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictive Models\n### Model Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"clfs = []\nseed = 3\n\nclfs.append((\"LogReg\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"LogReg\", LogisticRegression())])))\n\nclfs.append((\"XGBClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"XGB\", XGBClassifier())]))) \nclfs.append((\"KNN\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"KNN\", KNeighborsClassifier())]))) \n\nclfs.append((\"DecisionTreeClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"DecisionTrees\", DecisionTreeClassifier())]))) \n\nclfs.append((\"RandomForestClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"RandomForest\", RandomForestClassifier())]))) \n\nclfs.append((\"GradientBoostingClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"GradientBoosting\", GradientBoostingClassifier(max_features=15, \n                                                                       n_estimators=600))]))) \n\nclfs.append((\"RidgeClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"RidgeClassifier\", RidgeClassifier())])))\n\n\nclfs.append((\"ExtraTreesClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"ExtraTrees\", ExtraTreeClassifier())])))\n\nscoring = 'accuracy'\nn_folds = 10\nmsgs = []\nresults, names  = [], [] \n\nfor name, model  in clfs:\n    kfold = KFold(n_splits=n_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, y_train, \n                                 cv=kfold, scoring=scoring, n_jobs=-1)    \n    names.append(name)\n    results.append(cv_results)    \n    msg = \"%s: %f (+/- %f)\" % (name, cv_results.mean(),  \n                               cv_results.std())\n    msgs.append(msg)\n    print(msg)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> _Logistic Regression Outperforms others ML Models_."},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nx_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a model\nmodel = LogisticRegression()\n\n# feeding the training set into the model\nmodel.fit(x_train_scaled, y_train)\n\n# predicting the test set results\ny_pred = model.predict(X_test_scaled)\n\n# Calculating the accuracies\nprint(\"Training accuracy :\", model.score(x_train_scaled, y_train))\nprint(\"Testing accuarcy :\", model.score(X_test_scaled, y_test))\n\n# classification report\ncr = classification_report(y_test, y_pred)\nprint(cr)\n\n# confusion matrix \ncm = confusion_matrix(y_test, y_pred)\nplt.rcParams['figure.figsize'] = (5, 5)\nse.heatmap(cm, annot = True, cmap = 'winter')\nplt.title('Confusion Matrix', fontsize = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_clf = LogisticRegression(random_state = 42)\nparam_grid = {\n            'penalty' : ['l2','l1'],  \n            'C' : [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n            }\n\nCV_log_clf = GridSearchCV(estimator = log_clf, param_grid = param_grid , scoring = 'accuracy', verbose = 1, n_jobs = -1)\nCV_log_clf.fit(x_train_scaled, y_train)\n\nbest_parameters = CV_log_clf.best_params_\nprint('The best parameters for using this model is', best_parameters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          normalize = False,\n                          title = 'Confusion matrix\"',\n                          cmap = plt.cm.Blues) :\n    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation = 0)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) :\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment = 'center',\n                 color = 'white' if cm[i, j] > thresh else 'black')\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \n# Show metrics \ndef show_metrics():\n    tp = cm[1,1]\n    fn = cm[1,0]\n    fp = cm[0,1]\n    tn = cm[0,0]\n    print('Accuracy  =     {:.3f}'.format((tp+tn)/(tp+tn+fp+fn)))\n    print('Precision =     {:.3f}'.format(tp/(tp+fp)))\n    print('Recall    =     {:.3f}'.format(tp/(tp+fn)))\n    print('F1_score  =     {:.3f}'.format(2*(((tp/(tp+fp))*(tp/(tp+fn)))/\n                                                 ((tp/(tp+fp))+(tp/(tp+fn))))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cross_val_metrics(model) :\n    scores = ['accuracy', 'precision', 'recall']\n    for sc in scores:\n        scores = cross_val_score(model, X, y, cv = 5, scoring = sc)\n        print('[%s] : %0.5f (+/- %0.5f)'%(sc, scores.mean(), scores.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_roc():\n    plt.plot(fpr, tpr, label = 'ROC curve', linewidth = 2)\n    plt.plot([0,1],[0,1], 'k--', linewidth = 2)\n   # plt.xlim([0.0,0.001])\n   # plt.ylim([0.0,1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CV_log2_clf = LogisticRegression(C = best_parameters['C'], \n                                 penalty = best_parameters['penalty'], \n                                 random_state = 42)\n\n\nCV_log2_clf.fit(x_train_scaled, y_train)\n\ny_pred = CV_log2_clf.predict(X_test_scaled)\ny_score = CV_log2_clf.decision_function(X_test_scaled)\n# Confusion maxtrix & metrics\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the model to disk\nfilename = 'pickled_model.pkl'\npickle.dump(CV_log2_clf, open(filename, 'wb'))\n  \n# load the model from disk\npickle_model = pickle.load(open(filename, 'rb'))\nprint(pickle_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_metrics()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_val_metrics(CV_log2_clf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, t = roc_curve(y_test, y_score)\nplot_roc()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Deep learning Model with Keras"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(64,input_dim = 19,activation='relu'))\nmodel.add(Dense(32,activation='relu',init = 'uniform'))\nmodel.add(Dense(16,activation='relu',init = 'uniform'))\nmodel.add(Dense(1,activation = 'sigmoid'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss = 'binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)\nhistory=model.fit(x_train_scaled,y_train ,epochs=100,batch_size=128, validation_data=(X_test_scaled,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_, accuracy = model.evaluate(x_train_scaled, y_train)\n__, Accuracy = model.evaluate(X_test_scaled, y_test)\n\nprint('Accuracy Test: %.2f' % (accuracy*100))\nprint('accuracy Train: %.2f' % (Accuracy*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='test')\npyplot.legend()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## END"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}