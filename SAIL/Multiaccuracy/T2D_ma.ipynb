{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLife-Virtusa\n",
    "## SAIL - Multiaccuracy\n",
    "## Implementation of Multiaccuracy on EMR Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, recall_score, precision_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from t2dnn import ANN\n",
    "from sklearn.linear_model import Ridge\n",
    "seed=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('/home/ec2-user/SageMaker/vLife/Ruby/COHORT_SENSE_V1/balanced_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.rename(columns={'target':'target1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target2']=1-data['target1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Analysis\n",
    "### Train Multiaccuracy Model\n",
    "#### Train Test Split and initialize tf session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(data.drop(['target1','target2'],axis=1),data[['target1','target2']],test_size=0.2,random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputX=X_train.values\n",
    "inputY=y_train.values\n",
    "inputX_test=X_test.values\n",
    "inputY_test=y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.001\n",
    "training_epochs=2000\n",
    "display_step=50\n",
    "n_samples=inputY.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model3/model.ckpt\n",
      "Model restored.\n"
     ]
    }
   ],
   "source": [
    "net=ANN()\n",
    "sess=tf.Session()\n",
    "path='./model3/model.ckpt'\n",
    "net.load_model(sess,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008591089"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(net.cost,feed_dict={net.x:inputX,net.y_:inputY})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=sess.run(net.y,feed_dict={net.x:inputX_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "control = tf.cast(tf.greater(net.y[:,1],net.y[:,0]), tf.float32)\n",
    "noharm = [control, 1 - control, control + 1 - control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = net.y[:,1] - net.y[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.        , -0.99998665, -0.9999845 , -0.99999976, -0.99999267,\n",
       "       -1.        , -0.80649817, -1.        ,  0.6456487 , -0.9999969 ,\n",
       "        0.6390387 , -1.        , -1.        , -1.        ,  0.63763475,\n",
       "        0.6344669 ,  0.6508851 ,  0.6420013 , -0.99996376,  0.6395769 ,\n",
       "       -1.        , -1.        , -0.999996  , -0.99999595, -0.99974126,\n",
       "       -1.        , -0.59469175, -1.        , -0.99999934,  0.6385342 ,\n",
       "       -1.        ,  0.49974057,  0.64250267, -0.32492983, -1.        ,\n",
       "       -1.        , -0.99999976,  0.6347921 , -0.9475297 ,  0.64054084,\n",
       "       -1.        , -1.        ,  0.63920105,  0.6213882 ,  0.64490557,\n",
       "        0.63680613,  0.6455529 , -0.9997397 , -1.        , -1.        ,\n",
       "       -1.        , -0.99999875, -1.        , -1.        ,  0.6357789 ,\n",
       "       -0.99999905, -1.        ,  0.6485051 ,  0.642035  ,  0.64290535,\n",
       "        0.65638   ,  0.6375039 ,  0.6417881 , -0.99998087, -0.98518413,\n",
       "        0.64293855,  0.6421031 , -0.9999997 , -1.        ,  0.62824214,\n",
       "        0.64581275, -1.        ,  0.63231504, -1.        ,  0.6507017 ,\n",
       "        0.6412684 , -1.        ,  0.64231163, -1.        , -1.        ,\n",
       "        0.6337757 ,  0.6380395 , -1.        , -1.        , -0.887969  ,\n",
       "       -1.        ,  0.6387187 ,  0.642098  , -1.        ,  0.6400429 ,\n",
       "       -1.        ,  0.6389275 , -1.        ,  0.6402899 ,  0.63750446,\n",
       "        0.6413362 ,  0.6417658 ,  0.64028245, -0.9950954 , -1.        ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(logits,feed_dict={net.x:inputX_test,latent_ph:inputX_test})[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_T = 100\n",
    "thresh = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sess_run(result, x, sess):\n",
    "    num = x.shape[0]\n",
    "    num_batch = np.ceil(num/200).astype(int)\n",
    "    output = np.zeros(num)\n",
    "    for batch in range(num_batch):\n",
    "        output[batch*200:(batch+1)*200] = sess.run(result, feed_dict={net.x:inputX_test[batch*200:(batch+1)*200],latent_ph:inputX_test[batch*200:(batch+1)*200]})       \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res(p, y):\n",
    "    return y * ((p>=0.1)/(p + 1e-20) + (p<0.1) * (20 - 100  * p)) +(1-y) * ((p < 0.9)/(1 - p + 1e-20) + (p>=0.9) * (100 * p - 80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_ph = tf.placeholder(tf.float32, shape=(None, 124), name=\"latent_var\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"sub_3:0\", shape=(?,), dtype=float32)\n",
      "0 0 0.12297795177222579\n",
      "Tensor(\"sub_6:0\", shape=(?,), dtype=float32)\n",
      "1 0 -0.04980155246352291\n",
      "1 1 -0.04980155246352291\n",
      "1 2 -0.04980155246352291\n"
     ]
    }
   ],
   "source": [
    "## Train tensors using placeholders\n",
    "best_epoch, best_acc = -1,0\n",
    "coeffs = []\n",
    "for t in range(max_T):\n",
    "    control = tf.cast(tf.greater(net.y[:,1], net.y[:,0]), tf.float32)\n",
    "    noharm = [control, 1 - control, control + 1 - control]\n",
    "    print(logits)\n",
    "    probs_heldout = sess_run(tf.nn.sigmoid(logits), inputX_test[800:1600], sess=sess)\n",
    "    heldout_loss = np.mean(-inputY_test[:,0][800:1600] * np.log(probs_heldout + 1e-20) - (1-inputY_test[:,0][800:1600]) * np.log(1-probs_heldout + 1e-20))\n",
    "    heldout_acc =  np.mean((probs_heldout>0.5)==inputY_test[:,0][800:1600])\n",
    "    probs = sess_run(tf.nn.sigmoid(logits), inputX_test,sess)\n",
    "    val_loss = np.mean(-inputY_test[:,0] * np.log(probs + 1e-20) - (1 - inputY_test[:,0]) * np.log(1 - probs + 1e-20))\n",
    "    val_acc = np.mean((probs > 0.5) == inputY_test[:,0])\n",
    "    if heldout_acc > best_acc:\n",
    "        best_epoch = t\n",
    "        best_acc = heldout_acc\n",
    "        best_logits = logits\n",
    "    delta = res(probs,inputY_test[:,0])\n",
    "    residual = probs - inputY_test[:,0]\n",
    "    for i, s in enumerate(noharm):\n",
    "        temp_s = sess_run(noharm[i], inputX_test[:800], sess)\n",
    "        temp_s_heldout = sess_run(noharm[i], inputX_test[800:1600], sess)\n",
    "        samples1 = np.where(temp_s == 1)[0]\n",
    "        samples2 = np.where(temp_s_heldout == 1)[0]\n",
    "        clf = Ridge(alpha=1)\n",
    "        #clf.fit(inputX_test[:800],inputY_test[:800])\n",
    "        clf.fit(inputX_test[:800],delta[:800])\n",
    "        clf_prediction = clf.predict(inputX_test[800:1600])\n",
    "        #corr = np.mean(clf_prediction[:,0] * residual[800:1600])\n",
    "        corr = np.mean(clf_prediction * residual[800:1600])\n",
    "        print(t, i, corr)\n",
    "        if corr > 1e-4:\n",
    "            coeffs.append(clf.coef_)\n",
    "            h = (tf.matmul(latent_ph, tf.constant(np.expand_dims(clf.coef_,-1),\n",
    "                                                  dtype=tf.float32)) + clf.intercept_)\n",
    "            h=tf.reshape(h,[-1])\n",
    "            logits -= .1 * h * s\n",
    "            break\n",
    "    if i==2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Groups for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(189, 126)\n",
      "(706, 126)\n",
      "(660, 126)\n",
      "(51, 126)\n",
      "(9, 126)\n",
      "(6385, 126)\n"
     ]
    }
   ],
   "source": [
    "races=['race_asian','race_black','race_hispanic','race_native','race_other','race_white']\n",
    "for i in races:\n",
    "    print(data[data[i]==1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All_recall 0.9552795031055901 \n",
      "\n",
      "All_precision 0.9424019607843137 \n",
      "\n",
      "Asian_recall 0.9130434782608695 \n",
      "\n",
      "Asian_precision 0.7777777777777778 \n",
      "\n",
      "Black_recall 0.9425287356321839 \n",
      "\n",
      "Black_precision 0.9647058823529412 \n",
      "\n",
      "Hispanic_recall 0.9411764705882353 \n",
      "\n",
      "Hispanic_precision 0.9302325581395349 \n",
      "\n",
      "Native_recall 0.8333333333333334 \n",
      "\n",
      "Native_precision 0.625 \n",
      "\n",
      "Other_recall 0.0 \n",
      "\n",
      "Other_precision 0.0 \n",
      "\n",
      "White_recall 0.9619205298013245 \n",
      "\n",
      "White_precision 0.9524590163934427 \n",
      "\n",
      "Original:  all: 94.8 & race_asian: 78.4 & race_black: 93.8 & race_hispanic: 90.4 & race_native: 66.7 & race_other: 100.0 & race_white: 96.0 & \n"
     ]
    }
   ],
   "source": [
    "probs = sess_run(net.y[:,1] - net.y[:,0], X_test, sess)\n",
    "groups=['all','race_asian','race_black','race_hispanic','race_native','race_other','race_white']\n",
    "errs = []\n",
    "idxs = list(range(0,1600))\n",
    "print('All_recall',recall_score(y_test.iloc[idxs,0], (probs[idxs]<0.5)),'\\n')\n",
    "print('All_precision',precision_score(y_test.iloc[idxs,0], (probs[idxs]<0.5)),'\\n')\n",
    "errs.append(100 * np.mean((probs[idxs]>0.5)!=y_test.iloc[idxs,0]))\n",
    "idxs = np.where((X_test['race_asian']==1))[0]\n",
    "print('Asian_recall',recall_score(y_test.iloc[idxs,0], (probs[idxs]<0.5)),'\\n')\n",
    "print('Asian_precision',precision_score(y_test.iloc[idxs,0], (probs[idxs]<0.5)),'\\n')\n",
    "errs.append(100 * np.mean((probs[idxs]>0.5)!=y_test.iloc[idxs,0]))\n",
    "idxs = np.where((X_test['race_black']==1) )[0]\n",
    "print('Black_recall',recall_score(y_test.iloc[idxs,0], (probs[idxs]<0.5)),'\\n')\n",
    "print('Black_precision',precision_score(y_test.iloc[idxs,0], (probs[idxs]<0.5)),'\\n')\n",
    "errs.append(100 * np.mean((probs[idxs]>0.5)!=y_test.iloc[idxs,0]))\n",
    "idxs = np.where((X_test['race_hispanic']==1))[0]\n",
    "print('Hispanic_recall',recall_score(y_test.iloc[idxs,0], (probs[idxs]<0.5)),'\\n')\n",
    "print('Hispanic_precision',precision_score(y_test.iloc[idxs,0], (probs[idxs]<0.5)),'\\n')\n",
    "errs.append(100 * np.mean((probs[idxs]>0.5)!=y_test.iloc[idxs,0]))\n",
    "idxs = np.where((X_test['race_native']==1) )[0]\n",
    "print('Native_recall',recall_score(y_test.iloc[idxs,0], (probs[idxs]<0.5)),'\\n')\n",
    "print('Native_precision',precision_score(y_test.iloc[idxs,0], (probs[idxs]<0.5)),'\\n')\n",
    "errs.append(100 * np.mean((probs[idxs]>0.5)!=y_test.iloc[idxs,0]))\n",
    "idxs = np.where((X_test['race_other']==1) )[0]\n",
    "print('Other_recall',recall_score(y_test.iloc[idxs,0], (probs[idxs]<0.5)),'\\n')\n",
    "print('Other_precision',precision_score(y_test.iloc[idxs,0], (probs[idxs]<0.5)),'\\n')\n",
    "errs.append(100 * np.mean((probs[idxs]>0.5)!=y_test.iloc[idxs,0]))\n",
    "idxs = np.where((X_test['race_white']==1))[0]\n",
    "print('White_recall',recall_score(y_test.iloc[idxs,0], (probs[idxs]<0.5)),'\\n')\n",
    "print('White_precision',precision_score(y_test.iloc[idxs,0], (probs[idxs]<0.5)),'\\n')\n",
    "errs.append(100 * np.mean((probs[idxs]>0.5)!=y_test.iloc[idxs,0]))\n",
    "\n",
    "output = ''\n",
    "dict1={}\n",
    "for group, err in zip(groups, errs):\n",
    "    dict1[group]=str(round(err, 1))\n",
    "    output += group + ': ' + str(round(err, 1)) + ' & '\n",
    "print('Original: ', output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000000e+00, 3.4757381e-09],\n",
       "       [9.9999332e-01, 6.6863627e-06],\n",
       "       [9.9999225e-01, 7.7544291e-06],\n",
       "       [9.9999988e-01, 9.6157699e-08],\n",
       "       [9.9999630e-01, 3.6387466e-06],\n",
       "       [1.0000000e+00, 1.3357912e-08],\n",
       "       [9.0324908e-01, 9.6750937e-02],\n",
       "       [1.0000000e+00, 1.5975912e-10],\n",
       "       [2.4295757e-06, 9.9999762e-01],\n",
       "       [9.9999845e-01, 1.5272209e-06],\n",
       "       [2.7658317e-08, 1.0000000e+00],\n",
       "       [1.0000000e+00, 4.5145145e-11],\n",
       "       [1.0000000e+00, 5.4863593e-11],\n",
       "       [1.0000000e+00, 7.0759745e-11],\n",
       "       [2.0325375e-09, 1.0000000e+00],\n",
       "       [1.6953173e-08, 1.0000000e+00],\n",
       "       [1.1876276e-05, 9.9998808e-01],\n",
       "       [4.0650942e-08, 1.0000000e+00],\n",
       "       [9.9998188e-01, 1.8120552e-05],\n",
       "       [4.3626944e-10, 1.0000000e+00],\n",
       "       [1.0000000e+00, 3.8616825e-11],\n",
       "       [1.0000000e+00, 3.9547830e-11],\n",
       "       [9.9999797e-01, 1.9776669e-06],\n",
       "       [9.9999797e-01, 2.0322805e-06],\n",
       "       [9.9987066e-01, 1.2937262e-04],\n",
       "       [1.0000000e+00, 8.2303636e-11],\n",
       "       [7.9734588e-01, 2.0265412e-01],\n",
       "       [1.0000000e+00, 1.3604853e-09],\n",
       "       [9.9999964e-01, 3.0537581e-07],\n",
       "       [7.6490929e-09, 1.0000000e+00],\n",
       "       [1.0000000e+00, 4.0453205e-10],\n",
       "       [7.1339265e-02, 9.2866069e-01],\n",
       "       [2.0298951e-09, 1.0000000e+00],\n",
       "       [4.8265472e-01, 5.1734525e-01],\n",
       "       [1.0000000e+00, 3.6667967e-11],\n",
       "       [1.0000000e+00, 1.6209871e-09],\n",
       "       [9.9999988e-01, 9.2365724e-08],\n",
       "       [3.4597844e-05, 9.9996543e-01],\n",
       "       [9.7376484e-01, 2.6235169e-02],\n",
       "       [6.1090773e-06, 9.9999392e-01],\n",
       "       [1.0000000e+00, 5.8651597e-11],\n",
       "       [1.0000000e+00, 4.6953823e-11],\n",
       "       [1.1993561e-09, 1.0000000e+00],\n",
       "       [8.3757471e-03, 9.9162430e-01],\n",
       "       [1.8411824e-09, 1.0000000e+00],\n",
       "       [1.6250284e-07, 9.9999988e-01],\n",
       "       [3.8977413e-07, 9.9999964e-01],\n",
       "       [9.9986982e-01, 1.3012286e-04],\n",
       "       [1.0000000e+00, 4.9663357e-11],\n",
       "       [1.0000000e+00, 3.6428305e-11]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26894143, 0.26894405, 0.26894447, ..., 0.26894143, 0.26894143,\n",
       "       0.73049074])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess_run(tf.nn.sigmoid(best_logits), X_test, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All_recall 0.9478260869565217 \n",
      "\n",
      "All_precision 0.9490049751243781 \n",
      "\n",
      "Asian_recall 0.9130434782608695 \n",
      "\n",
      "Asian_precision 0.8076923076923077 \n",
      "\n",
      "Black_recall 0.9425287356321839 \n",
      "\n",
      "Black_precision 0.9647058823529412 \n",
      "\n",
      "Hispanic_recall 0.9294117647058824 \n",
      "\n",
      "Hispanic_precision 0.9404761904761905 \n",
      "\n",
      "Native_recall 0.8333333333333334 \n",
      "\n",
      "Native_precision 0.625 \n",
      "\n",
      "Other_recall 0.0 \n",
      "\n",
      "Other_precision 0.0 \n",
      "\n",
      "White_recall 0.9536423841059603 \n",
      "\n",
      "White_precision 0.9584026622296173 \n",
      "\n",
      "MultiAccuracy Boost:  all: 94.8 & race_asian: 81.1 & race_black: 93.8 & race_hispanic: 90.4 & race_native: 66.7 & race_other: 100.0 & race_white: 95.9 & \n"
     ]
    }
   ],
   "source": [
    "probs = sess_run(tf.nn.sigmoid(best_logits), X_test, sess)\n",
    "groups=['all','race_asian','race_black','race_hispanic','race_native','race_other','race_white']\n",
    "#groups = ['all', 'F', 'M', 'B', 'N', 'BF', 'BM', 'NF', 'NM']\n",
    "errs = []\n",
    "idxs = list(range(0,1600))\n",
    "print('All_recall',recall_score(y_test.iloc[idxs,0], (probs[idxs]<0.5)),'\\n')\n",
    "print('All_precision',precision_score(y_test.iloc[idxs,0], (probs[idxs]<0.5)),'\\n')\n",
    "errs.append(100 * np.mean((probs[idxs]>0.5)!=y_test.iloc[idxs,0]))\n",
    "idxs = np.where((X_test['race_asian']==1))[0]\n",
    "#print(idxs)\n",
    "print('Asian_recall',recall_score(y_test.iloc[idxs,0], (probs[idxs]<0.5)),'\\n')\n",
    "print('Asian_precision',precision_score(y_test.iloc[idxs,0], (probs[idxs]<0.5)),'\\n')\n",
    "errs.append(100 * np.mean((probs[idxs]>0.5)!=y_test.iloc[idxs,0]))\n",
    "idxs = np.where((X_test['race_black']==1) )[0]\n",
    "print('Black_recall',recall_score(y_test.iloc[idxs,0], (probs[idxs]<0.5)),'\\n')\n",
    "print('Black_precision',precision_score(y_test.iloc[idxs,0], (probs[idxs]<0.5)),'\\n')\n",
    "errs.append(100 * np.mean((probs[idxs]>0.5)!=y_test.iloc[idxs,0]))\n",
    "idxs = np.where((X_test['race_hispanic']==1))[0]\n",
    "print('Hispanic_recall',recall_score(y_test.iloc[idxs,0], (probs[idxs]<0.5)),'\\n')\n",
    "print('Hispanic_precision',precision_score(y_test.iloc[idxs,0], (probs[idxs]<0.5)),'\\n')\n",
    "errs.append(100 * np.mean((probs[idxs]>0.5)!=y_test.iloc[idxs,0]))\n",
    "idxs = np.where((X_test['race_native']==1) )[0]\n",
    "print('Native_recall',recall_score(y_test.iloc[idxs,0], (probs[idxs]<0.5)),'\\n')\n",
    "print('Native_precision',precision_score(y_test.iloc[idxs,0], (probs[idxs]<0.5)),'\\n')\n",
    "errs.append(100 * np.mean((probs[idxs]>0.5)!=y_test.iloc[idxs,0]))\n",
    "idxs = np.where((X_test['race_other']==1) )[0]\n",
    "print('Other_recall',recall_score(y_test.iloc[idxs,0], (probs[idxs]<0.5)),'\\n')\n",
    "print('Other_precision',precision_score(y_test.iloc[idxs,0], (probs[idxs]<0.5)),'\\n')\n",
    "errs.append(100 * np.mean((probs[idxs]>0.5)!=y_test.iloc[idxs,0]))\n",
    "idxs = np.where((X_test['race_white']==1))[0]\n",
    "print('White_recall',recall_score(y_test.iloc[idxs,0], (probs[idxs]<0.5)),'\\n')\n",
    "print('White_precision',precision_score(y_test.iloc[idxs,0], (probs[idxs]<0.5)),'\\n')\n",
    "errs.append(100 * np.mean((probs[idxs]>0.5)!=y_test.iloc[idxs,0]))\n",
    "# idxs = np.where((X_test['race_white']==1) * (X_test['gender']==1))[0]\n",
    "# errs.append(100 * np.mean((probs[idxs]>0.5)!=y_test.iloc[idxs,0]))\n",
    "# idxs = np.where((X_test['race_white']==0) * (X_test['gender']==0))[0]\n",
    "# errs.append(100 * np.mean((probs[idxs]>0.5)!=y_test.iloc[idxs,0]))\n",
    "# idxs = np.where((X_test['race_white']==0) * (X_test['gender']==1))[0]\n",
    "# errs.append(100 * np.mean((probs[idxs]>0.5)!=y_test.iloc[idxs,0]))\n",
    "output = ''\n",
    "for group, err in zip(groups, errs):\n",
    "    output += group + ': ' + str(round(err, 1)) + ' & '\n",
    "print('MultiAccuracy Boost: ', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
